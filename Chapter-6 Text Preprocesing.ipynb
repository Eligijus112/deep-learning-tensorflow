{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Procesing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Natural Language Processing or NLP is a field of Artificial Intelligence that gives the machines the ability to read, understand and derive meaning from human languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is used in businesses throughtout the world: chatbots, sentiment analysis, word embeddings, text classification and many other uses have found its way into the industry. But how does a machine view text? \n",
    "\n",
    "We know that algorithms use numbers to optimize loss functions thus texts need to be converted to numbers in a smart way for computers to gain meaning from them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocesing using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create some simple sentences: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"My dog is a very good boy!\",\n",
    "    \"I do not think my cat likes me, therefore, I don't know if I like her\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the introduction, the first thing to do is to convert the given texts to numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text index and sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tokenizer class from tensorflow does all the heavy lifting for us. The method **fit_on_texts()** removes all punctuations, lowercases the words and creates the word index dictionary where each unique word is assigned a unique integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'i': 2, 'my': 3, 'dog': 4, 'is': 5, 'a': 6, 'very': 7, 'good': 8, 'boy': 9, 'do': 10, 'not': 11, 'think': 12, 'cat': 13, 'likes': 14, 'me': 15, 'therefore': 16, \"don't\": 17, 'know': 18, 'if': 19, 'like': 20, 'her': 21}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Initiating the class\n",
    "# The oov_token is used to encode words that were not seen in the training set (out of vocabulary)\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "\n",
    "# Fitting on our texts\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Printing out the word index\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word index is sorted by the frequency of word appearance so the first words appearing in the index are the most frequent. We can view the word count using the tokenizer.word_counts object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('my', 2), ('dog', 1), ('is', 1), ('a', 1), ('very', 1), ('good', 1), ('boy', 1), ('i', 3), ('do', 1), ('not', 1), ('think', 1), ('cat', 1), ('likes', 1), ('me', 1), ('therefore', 1), (\"don't\", 1), ('know', 1), ('if', 1), ('like', 1), ('her', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have integers representing words. The next step is to convert the sentences to sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '<OOV>', 2: 'i', 3: 'my', 4: 'dog', 5: 'is', 6: 'a', 7: 'very', 8: 'good', 9: 'boy', 10: 'do', 11: 'not', 12: 'think', 13: 'cat', 14: 'likes', 15: 'me', 16: 'therefore', 17: \"don't\", 18: 'know', 19: 'if', 20: 'like', 21: 'her'}\n"
     ]
    }
   ],
   "source": [
    "# Helper functions to decode integers into words \n",
    "index_word = dict((v,k) for k,v in tokenizer.word_index.items())\n",
    "\n",
    "print(index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 4, 5, 6, 7, 8, 9], [2, 10, 11, 12, 3, 13, 14, 15, 16, 2, 17, 18, 19, 2, 20, 21]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two lists represent the two original sentences that were created. Each integer represent a word from the word index. Let us see what happens when new words appear in sentences and we use the already trained tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 13, 1, 3, 4, 1, 8, 1]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = ['My cat and my dog are good buddies'] \n",
    "\n",
    "# Converting to sequence \n",
    "print(tokenizer.texts_to_sequences(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'cat', '<OOV>', 'my', 'dog', '<OOV>', 'good', '<OOV>']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting to characters \n",
    "[index_word.get(x) for x in tokenizer.texts_to_sequences(test_text)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that because the words 'and', 'are' and 'buddies' were not in our training corpus, they are labeled as '<OOV>' because that is what we defined for the words that were not in the training vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training sentences transformed into integer sequances we have the problem of them beeing of different lenghts. Recall that for most machine learning algorithms, all the observations needs to have the same number of columns, even if some features are missing. \n",
    "\n",
    "In our example case, the first sentence has 7 words and the second sentence has 16 words. To create a matrix with 16 columns and 2 rows we use the **pad_sequence** method from tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  3  4  5  6  7  8  9]\n",
      " [ 2 10 11 12  3 13 14 15 16  2 17 18 19  2 20 21]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Padding the created sentences \n",
    "padded = pad_sequences(sequences)\n",
    "\n",
    "# Printing out the results\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the **pad_sequences()** method finds the longest sequence and pads to the length of the longest sentence. As we can see from the results, there are a bunch of zeroes added to the front of the first sentence in order to create a tidy structure of the matrix. \n",
    "\n",
    "I personaly like when the zeroes are added to the back. It can be achieved by using the **padding=post** parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  4  5  6  7  8  9  0  0  0  0  0  0  0  0  0]\n",
      " [ 2 10 11 12  3 13 14 15 16  2 17 18 19  2 20 21]]\n"
     ]
    }
   ],
   "source": [
    "print(pad_sequences(sequences, padding='post'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example using Twitter data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets wrap everything up using a real world example using twitter data regarding disaster tweets. In this dataset there are two types of tweets: ones regarding natural disasters and the other ones regarding not disasters.\n",
    "\n",
    "The data can be downloaded here: https://www.kaggle.com/c/nlp-getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets: 7613\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "tweets = pd.read_csv(f'{os.getcwd()}/NLP/twitter-disaster-tweets/tweets.csv')['text'].values.tolist()\n",
    "\n",
    "print(f'Total tweets: {len(tweets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n",
       " 'Forest fire near La Ronge Sask. Canada',\n",
       " \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\",\n",
       " '13,000 people receive #wildfires evacuation orders in California ',\n",
       " 'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school ',\n",
       " '#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires',\n",
       " '#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas',\n",
       " \"I'm on top of the hill and I can see a fire in the woods...\",\n",
       " \"There's an emergency evacuation happening now in the building across the street\",\n",
       " \"I'm afraid that the tornado is coming to our area...\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample of some tweets\n",
    "tweets[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initating the tokenizer\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "\n",
    "# Fitting on our texts\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "# Text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "\n",
    "# Padding\n",
    "tweet_matrix = pad_sequences(sequences, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 124, 4507,   27, ...,    0,    0,    0],\n",
       "       [ 186,   47,  227, ...,    0,    0,    0],\n",
       "       [  43, 1702, 1859, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [2733, 2333,  678, ...,    0,    0,    0],\n",
       "       [  79, 1115,   41, ...,    0,    0,    0],\n",
       "       [   5,  203,   57, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 33)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the vocabulary: 21719\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of unique words in the vocabulary: {len(tokenizer.word_index)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-tf",
   "language": "python",
   "name": "deep-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
